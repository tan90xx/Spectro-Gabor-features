{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d15b9be-a98b-484d-814d-f0fda953c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnAudio import features\n",
    "from scipy.io import wavfile\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from glob import glob\n",
    "from net_factory import get_network_fn\n",
    "from torchsummary import summary\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
    "import torch.nn.functional as F\n",
    "from utils import accuracy, AverageMeter, save_checkpoint, visualize_graph, get_parameters_size\n",
    "from tensorboardX import SummaryWriter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ddc2f3b-6f5a-42f6-a349-2b9a38c1a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './DeepShip_train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a04c9a4-935e-4030-b831-f0845dfe85e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_paths = glob('{}/**'.format(path), recursive=True)\n",
    "wav_paths = [x.replace(os.sep, '/') for x in wav_paths if '.wav' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0377e19-f7ad-4d27-9fe2-cb9619dad38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted([f for f in os.listdir(path) if not f.startswith('.')])\n",
    "le = LabelEncoder()\n",
    "le.fit(classes)\n",
    "labels = [os.path.split(x)[0].split('/')[-1] for x in wav_paths]\n",
    "labels = le.transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab48b247-5d70-4afd-8e42-3def2e55eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Class to load the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, auList, labelList):\n",
    "        \"\"\"\n",
    "        :param auList: audio list (Note that these lists have been processed and pickled using the loadData.py)\n",
    "        :param labelList: label list (Note that these lists have been processed and pickled using the loadData.py)\n",
    "        \"\"\"\n",
    "        self.auList = auList\n",
    "        self.labelList = labelList\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.auList)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        :param idx: Index of the image file\n",
    "        :return: returns the image and corresponding label file.\n",
    "        \"\"\"\n",
    "        _, audio = wavfile.read(self.auList[idx])\n",
    "        label = self.labelList[idx]\n",
    "\n",
    "        return (audio, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d979d3c-aac2-438f-b241-9fd307aab711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 96000])\n",
      "tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "# 数据准备\n",
    "data_loader = DataLoader(MyDataset(wav_paths[:2], labels[:2]), batch_size=2, num_workers=8, pin_memory=True, shuffle=False)\n",
    "for batch_idx, (data, target) in enumerate(data_loader):\n",
    "    print(data.shape)\n",
    "    print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "723dc7ff-a344-4236-ba98-b5fd12dd1695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STFT kernels created, time used = 0.1453 seconds\n",
      "STFT filter created, time used = 0.0044 seconds\n",
      "Mel filter created, time used = 0.0044 seconds\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "              STFT-1            [-1, 1025, 188]               0\n",
      "    MelSpectrogram-2             [-1, 128, 188]               0\n",
      "           GaborYu-3          [-1, 1, 128, 188]              36\n",
      "            Conv2d-4         [-1, 20, 126, 186]             180\n",
      "              ReLU-5         [-1, 20, 126, 186]               0\n",
      "         MaxPool2d-6           [-1, 20, 63, 93]               0\n",
      "            Conv2d-7           [-1, 40, 63, 93]          20,000\n",
      "              ReLU-8           [-1, 40, 63, 93]               0\n",
      "         MaxPool2d-9           [-1, 40, 31, 46]               0\n",
      "           Conv2d-10           [-1, 80, 31, 46]          80,000\n",
      "             ReLU-11           [-1, 80, 31, 46]               0\n",
      "        MaxPool2d-12           [-1, 80, 15, 23]               0\n",
      "           Conv2d-13          [-1, 160, 15, 23]         320,000\n",
      "             ReLU-14          [-1, 160, 15, 23]               0\n",
      "           Conv2d-15          [-1, 320, 15, 23]       1,280,000\n",
      "             ReLU-16          [-1, 320, 15, 23]               0\n",
      "           Linear-17                    [-1, 4]           1,284\n",
      "================================================================\n",
      "Total params: 1,701,500\n",
      "Trainable params: 1,701,500\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.37\n",
      "Forward/backward pass size (MB): 18.37\n",
      "Params size (MB): 6.49\n",
      "Estimated Total Size (MB): 25.23\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = get_network_fn('audioGabor')\n",
    "device = \"cuda\"\n",
    "model = model.to(device)\n",
    "summary(model, input_size=(1, 96000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46a8822e-134c-49bf-93bd-fa1256be79e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01, momentum=0.9, weight_decay=3e-05)#code\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)#code--10, 0.5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce1f11e7-0d66-497c-b4a8-8cf6fe818c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_prec1 = 0\n",
    "writer = SummaryWriter(comment='_'+'audioGabor'+'_')\n",
    "iteration = 0\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    global iteration\n",
    "    st = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        prec1, = accuracy(output, target)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accuracy: {:.2f}'.format(\n",
    "                epoch, batch_idx * len(data), len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader), loss.item(), prec1.item()))\n",
    "            writer.add_scalar('Loss/Train', loss.item(), iteration)\n",
    "            writer.add_scalar('Accuracy/Train', prec1, iteration)\n",
    "    epoch_time = time.time() - st\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    print('Epoch time:{:0.2f}s'.format(epoch_time),  '\tlearning-rate:', lr)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5ca6b4f-32f2-4dcf-a0cf-838b2ea2b430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Train Epoch: 1 [0/8 (0%)]\tLoss: 2.330684, Accuracy: 0.00\n",
      "Epoch time:0.48s \tlearning-rate: 0.01\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 2 [0/8 (0%)]\tLoss: 2.213097, Accuracy: 0.00\n",
      "Epoch time:0.45s \tlearning-rate: 0.01\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 3 [0/8 (0%)]\tLoss: 5.969714, Accuracy: 0.00\n",
      "Epoch time:0.44s \tlearning-rate: 0.01\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 4 [0/8 (0%)]\tLoss: 1.792215, Accuracy: 100.00\n",
      "Epoch time:0.44s \tlearning-rate: 0.01\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 5 [0/8 (0%)]\tLoss: 1.257665, Accuracy: 100.00\n",
      "Epoch time:0.46s \tlearning-rate: 0.01\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 6 [0/8 (0%)]\tLoss: 1.275289, Accuracy: 100.00\n",
      "Epoch time:0.51s \tlearning-rate: 0.01\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 7 [0/8 (0%)]\tLoss: 2.293290, Accuracy: 0.00\n",
      "Epoch time:0.53s \tlearning-rate: 0.01\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 8 [0/8 (0%)]\tLoss: 1.497003, Accuracy: 100.00\n",
      "Epoch time:0.45s \tlearning-rate: 0.01\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 9 [0/8 (0%)]\tLoss: 0.946814, Accuracy: 100.00\n",
      "Epoch time:0.46s \tlearning-rate: 0.001\n",
      "------------------------------------------------------------------------\n",
      "Train Epoch: 10 [0/8 (0%)]\tLoss: 0.731771, Accuracy: 100.00\n",
      "Epoch time:0.51s \tlearning-rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "for epoch in range(epochs):\n",
    "    print('------------------------------------------------------------------------')\n",
    "    train(epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "393b7881-a463-4488-a713-a498727de42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STFT kernels created, time used = 0.1127 seconds\n",
      "STFT filter created, time used = 0.0048 seconds\n",
      "Mel filter created, time used = 0.0048 seconds\n",
      "dict_keys(['epoch', 'state_dict', 'best_prec1', 'optimizer'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_network_fn('audioGabor')\n",
    "checkpoint = torch.load('model_best.pth.tar')#,map_location='cpu'\n",
    "print(checkpoint.keys())\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04534b29-04e2-4411-a1d8-99cb2df4da6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 2]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "outputs = []\n",
    "\n",
    "maxk = max((1,))\n",
    "for batch,_ in data_loader:\n",
    "    # Every time forward is called, attention maps will be generated and saved in the directory \"attention_maps\"\n",
    "    output = model(batch)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba9038e-5f06-434a-b040-8081403146fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
